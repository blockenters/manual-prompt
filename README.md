# RAG 강의용!!

# PDF 문서 기반 질의응답 시스템

이 프로젝트는 "선진기업복지 업무메뉴얼" PDF 문서를 기반으로 질의응답을 제공하는 시스템입니다. 사용자는 PDF 문서에 대한 질문을 입력하면, 시스템은 해당 문서를 분석하여 정확한 답변을 제공합니다.

앱 주소: https://manual-prompt-i54gigmdkaicinzbvvfasm.streamlit.app

## 기능

* PDF 문서 로드 및 분석
* 사용자 질문 입력 및 처리
* 문서 기반으로 답변 생성 및 제공
* 검색된 관련 문서 및 소스 확인 기능
* 높은 정확도의 문서 기반 답변 제공

## 기술 스택

* Llama Index: 문서 기반 질의응답 엔진
* Hugging Face: 언어 모델
* OpenAI: 고성능 임베딩 모델 (text-embedding-3-small)
* Torch: 딥 러닝 모델 구현
* Streamlit: 웹 애플리케이션 개발
* PyPDF: PDF 문서 읽기 및 분석

## 시스템 특징

### 1. PDF 직접 처리 및 인덱싱
- PDF 파일을 직접 로드하고 텍스트를 추출합니다.
- 페이지 번호 메타데이터를 포함하여 각 청크의 출처를 추적합니다.

### 2. 최적화된 청킹(Chunking) 전략
- 더 작은 청크 크기(256 토큰)로 세분화하여 정확한 검색 결과를 얻습니다.
- 50 토큰의 오버랩을 추가하여 문맥 손실을 방지합니다.
- 문단과 문장 단위로 청킹하여 의미적 일관성을 유지합니다.
- 이전/다음 청크 관계를 포함하여 관련 컨텍스트를 유지합니다.

### 3. 강화된 검색 및 응답 생성
- OpenAI의 text-embedding-3-small 모델을 사용하여 임베딩 품질 향상
- 상위 5개의 관련 문서를 검색합니다.
- 유사도 0.7 이상인 문서만 고려하여 관련성이 낮은 내용을 필터링합니다.
- `refine` 응답 모드를 사용하여 검색된 모든 문서를 순차적으로 처리합니다.

### 4. 향상된 사용자 경험 및 투명성
- 검색된 관련 문서와 그 유사도 점수를 확인할 수 있는 확장 패널을 제공합니다.
- 답변 생성에 사용된 소스 문서를 확인할 수 있어 투명성이 향상되었습니다.
- 시스템 프롬프트를 강화하여 문서에 없는 내용은 답변하지 않도록 했습니다.

### 5. 효율적인 인덱스 관리
- 인덱스를 생성하고 로컬에 저장하여 재사용합니다.
- 이미 인덱스가 있으면 로드하고, 없으면 새로 생성합니다.

### 6. OpenAI 임베딩 모델 적용
- Hugging Face 임베딩 모델에서 OpenAI의 text-embedding-3-small 모델로 변경
- 1536차원 벡터로 더 정확한 의미적 표현 가능
- 배치 처리를 통한 효율적인 임베딩 생성
- 한국어 문서에 대한 더 나은 이해와 검색 성능 제공

## 실행 방법

1. 이 프로젝트를 로컬에 클론합니다.
2. 필요한 라이브러리를 설치합니다. (`pip install -r requirements.txt`)
3. OpenAI API 키를 환경 변수로 설정합니다. (`export OPENAI_API_KEY=your_api_key`)
4. Hugging Face API 토큰을 환경 변수로 설정합니다. (`export HUGGINGFACE_API_TOKEN=your_token`)
5. `app.py` 파일을 실행합니다. (`streamlit run app.py`)
6. 웹 브라우저에서 로컬 Streamlit 서버에 접속합니다.
7. 질문을 입력하여 PDF 문서에 대한 답변을 받습니다.

## 주의 사항

* 이 시스템은 한국어로만 작동합니다.
* PDF 문서의 내용이 정확하고 완전해야 합니다.
* 시스템의 성능은 문서의 크기 및 복잡도에 따라 다를 수 있습니다.
* OpenAI API 사용량에 따라 비용이 발생할 수 있습니다.
